<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration supports_final="true">
    <!-- file system properties -->
    <property>
        <name>dfs.datanode.handler.count</name>
        <value>512</value>
    </property>
    <property>
        <name>dfs.namenode.service.handler.count</name>
        <value>512</value>
    </property>

    <property>
        <name>dfs.namenode.name.dir</name>
        <value>{{dfs_name_dir}}</value>
        <display-name>NameNode directories</display-name>
        <description>Determines where on the local filesystem the DFS name node
            should store the name table. If this is a comma-delimited list
            of directories then the name table is replicated in all of the
            directories, for redundancy.
        </description>
        <final>true</final>
        <value-attributes>
            <type>directories</type>
            <overridable>false</overridable>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
        <display-name>WebHDFS enabled</display-name>
        <description>Whether to enable WebHDFS feature</description>
        <final>true</final>
        <value-attributes>
            <type>boolean</type>
            <overridable>false</overridable>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.failed.volumes.tolerated</name>
        <value>2</value>
        <description>Number of failed disks a DataNode would tolerate before it stops offering service</description>
        <final>true</final>
        <display-name>DataNode failed disk tolerance</display-name>
        <value-attributes>
            <type>int</type>
            <minimum>0</minimum>
            <maximum>2</maximum>
            <increment-step>1</increment-step>
        </value-attributes>
        <depends-on>
            <property>
                <type>hdfs-site</type>
                <name>dfs.datanode.data.dir</name>
            </property>
        </depends-on>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>{{dfs_data_dirs}}</value>
        <display-name>DataNode directories</display-name>
        <description>Determines where on the local filesystem an DFS data node
            should store its blocks. If this is a comma-delimited
            list of directories, then data will be stored in all named
            directories, typically on different devices.
            Directories that do not exist are ignored.
        </description>
        <final>true</final>
        <value-attributes>
            <type>directories</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.hosts.exclude</name>
        <value>/etc/hadoop/dfs.exclude</value>
        <description>Names a file that contains a list of hosts that are
            not permitted to connect to the namenode. The full pathname of the
            file must be specified. If the value is empty, no hosts are
            excluded.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>{{dfs_checkpoint_dirs}}</value>
        <display-name>SecondaryNameNode Checkpoint directories</display-name>
        <description>Determines where on the local filesystem the DFS secondary
            name node should store the temporary images to merge.
            If this is a comma-delimited list of directories then the image is
            replicated in all of the directories for redundancy.
        </description>
        <value-attributes>
            <type>directories</type>
            <overridable>false</overridable>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.edits.dir</name>
        <display-name>NameNode Checkpoint Edits directory</display-name>
        <value>${dfs.namenode.checkpoint.dir}</value>
        <description>Determines where on the local filesystem the DFS secondary
            name node should store the temporary edits to merge.
            If this is a comma-delimited list of directories then the edits are
            replicated in all of the directories for redundancy.
            Default value is same as dfs.namenode.checkpoint.dir
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.period</name>
        <value>21600</value>
        <display-name>HDFS Maximum Checkpoint Delay</display-name>
        <description>The number of seconds between two periodic checkpoints.</description>
        <value-attributes>
            <type>int</type>
            <unit>seconds</unit>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.txns</name>
        <value>1000000</value>
        <description>The Secondary NameNode or CheckpointNode will create a checkpoint
            of the namespace every 'dfs.namenode.checkpoint.txns' transactions,
            regardless of whether 'dfs.namenode.checkpoint.period' has expired.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.replication.max</name>
        <value>50</value>
        <description>Maximal block replication.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
        <display-name>Block replication</display-name>
        <description>Default block replication.
        </description>
        <value-attributes>
            <type>int</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.heartbeat.interval</name>
        <value>3</value>
        <description>Determines datanode heartbeat interval in seconds.</description>
        <on-ambari-upgrade add="false"/>
        <supported-refresh-commands>
            <refresh-command componentName="NAMENODE" command="reload_configs"/>
        </supported-refresh-commands>
    </property>
    <property>
        <name>dfs.namenode.safemode.threshold-pct</name>
        <value>0.999</value>
        <description>
            Specifies the percentage of blocks that should satisfy
            the minimal replication requirement defined by dfs.namenode.replication.min.
            Values less than or equal to 0 mean not to start in safe mode.
            Values greater than 1 will make safe mode permanent.
        </description>
        <display-name>Minimum replicated blocks %</display-name>
        <value-attributes>
            <type>float</type>
            <minimum>0.990</minimum>
            <maximum>1.000</maximum>
            <increment-step>0.001</increment-step>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.balance.bandwidthPerSec</name>
        <value>600m</value>
        <description>
            Specifies the maximum amount of bandwidth that each datanode
            can utilize for the balancing purpose in term of
            the number of bytes per second.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.https.port</name>
        <value>9871</value>
        <description>
            This property is used by HftpFileSystem.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:9866</value>
        <description>
            The datanode server address and port for data transfer.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:9864</value>
        <description>
            The datanode http server address and port.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.https.address</name>
        <value>0.0.0.0:9685</value>
        <description>
            The datanode https server address and port.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>268435456</value>
        <description>The default block size for new files.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>0.0.0.0:9870</value>
        <description>The name of the default file system. Either the
            literal string "local" or a host:port for HDFS.
        </description>
        <final>true</final>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.rpc-address</name>
        <value>localhost:8020</value>
        <description>RPC address that handles all clients requests.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.du.reserved</name>
        <!-- cluster variant -->
        <value>1073741824</value>
        <display-name>Reserved space for HDFS</display-name>
        <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
        </description>
        <value-attributes>
            <type>int</type>
            <unit>bytes</unit>
        </value-attributes>
        <depends-on>
            <property>
                <type>hdfs-site</type>
                <name>dfs.datanode.data.dir</name>
            </property>
        </depends-on>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.ipc.address</name>
        <value>0.0.0.0:9867</value>
        <description>
            The datanode ipc server address and port.
            If the port is 0 then the server will start on a free port.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.blockreport.initialDelay</name>
        <value>120</value>
        <description>Delay for first block report in seconds.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>16384</value>
        <description>Specifies the maximum number of threads to use for transferring data in and out of the datanode.
        </description>
        <display-name>DataNode max data transfer threads</display-name>
        <value-attributes>
            <type>int</type>
            <minimum>0</minimum>
            <maximum>48000</maximum>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <!-- Permissions configuration -->
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>022</value>
        <description>
            The octal umask used when creating files and directories.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
        <description>
            If "true", enable permission checking in HDFS.
            If "false", permission checking is turned off,
            but all other behavior is unchanged.
            Switching from one parameter value to the other does not change the mode,
            owner or group of files or directories.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.permissions.superusergroup</name>
        <value>hdfs</value>
        <property-type>GROUP</property-type>
        <description>The name of the group of super-users.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>256</value>
        <description>Added to grow Queue size so that more client connections are allowed</description>
        <display-name>NameNode Server threads</display-name>
        <value-attributes>
            <type>int</type>
            <minimum>1</minimum>
            <maximum>1024</maximum>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.block.access.token.enable</name>
        <value>true</value>
        <description>
            If "true", access tokens are used as capabilities for accessing datanodes.
            If "false", no access tokens are checked on accessing datanodes.
        </description>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <!-- cluster variant -->
        <name>dfs.namenode.secondary.http-address</name>
        <value>localhost:9868</value>
        <description>Address of secondary namenode web server</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.https-address</name>
        <value>0.0.0.0:9871</value>
        <description>The https address where namenode binds</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.datanode.data.dir.perm</name>
        <value>750</value>
        <display-name>DataNode directories permission</display-name>
        <description>The permissions that should be there on dfs.datanode.data.dir
            directories. The datanode will not come up if the permissions are
            different on existing dfs.datanode.data.dir directories. If the directories
            don't exist, they will be created with this permission.
        </description>
        <value-attributes>
            <type>int</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.accesstime.precision</name>
        <value>0</value>
        <display-name>Access time precision</display-name>
        <description>The access time for HDFS file is precise up to this value.
            The default value is 1 hour. Setting a value of 0 disables
            access times for HDFS.
        </description>
        <value-attributes>
            <type>int</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.cluster.administrators</name>
        <value>hdfs</value>
        <description>ACL for who all can view the default servlets in the HDFS</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.avoid.read.stale.datanode</name>
        <value>true</value>
        <description>
            Indicate whether or not to avoid reading from stale datanodes whose
            heartbeat messages have not been received by the namenode for more than a
            specified time interval.
        </description>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.avoid.write.stale.datanode</name>
        <value>true</value>
        <description>
            Indicate whether or not to avoid writing to stale datanodes whose
            heartbeat messages have not been received by the namenode for more than a
            specified time interval.
        </description>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.write.stale.datanode.ratio</name>
        <value>1.0f</value>
        <description>When the ratio of number stale datanodes to total datanodes marked is greater
            than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.stale.datanode.interval</name>
        <value>30000</value>
        <description>Datanode is stale after not getting a heartbeat in this interval in ms</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>true</value>
        <description>Set to true to enable support for HDFS ACLs (Access Control Lists).</description>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.journalnode.http-address</name>
        <value>0.0.0.0:8480</value>
        <description>The address and port the JournalNode web UI listens on.
            If the port is 0 then the server will start on a free port.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.journalnode.https-address</name>
        <value>0.0.0.0:8481</value>
        <description>The address and port the JournalNode HTTPS server listens on.
            If the port is 0 then the server will start on a free port.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <display-name>JournalNode Edits directory</display-name>
        <value>/data1/hadoop/hdfs/journal</value>
        <description>The path where the JournalNode daemon will store its local state.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <!-- HDFS Short-Circuit Local Reads -->
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
        <display-name>HDFS Short-circuit read</display-name>
        <description>
            This configuration parameter turns on short-circuit local reads.
        </description>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/lib/hadoop-hdfs/dn_socket</value>
        <description>
            This is a path to a UNIX domain socket that will be used for communication between the DataNode and local
            HDFS clients.
            If the string "_PORT" is present in this path, it will be replaced by the TCP port of the DataNode.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit.streams.cache.size</name>
        <value>4096</value>
        <description>
            The DFSClient maintains a cache of recently opened file descriptors. This
            parameter controls the size of that cache. Setting this higher will use
            more file descriptors, but potentially provide better performance on
            workloads involving lots of seeks.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.name.dir.restore</name>
        <value>true</value>
        <description>Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir.
            When enabled, a recovery of any failed directory is attempted during checkpoint.
        </description>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.http.policy</name>
        <value>HTTP_ONLY</value>
        <description>
            Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons:
            The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY :
            Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https
        </description>
        <on-ambari-upgrade add="false"/>
    </property>

    <property>
        <name>dfs.namenode.audit.log.async</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <description>Whether to enable async auditlog</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.fslock.fair</name>
        <value>false</value>
        <description>Whether fsLock is fair</description>
        <on-ambari-upgrade add="false"/>
    </property>

    <!-- These configs were inherited from HDP 2.2 -->
    <property>
        <name>dfs.namenode.startup.delay.block.deletion.sec</name>
        <value>3600</value>
        <description>
            The delay in seconds at which we will pause the blocks deletion
            after Namenode startup. By default it's disabled.
            In the case a directory has large number of directories and files are
            deleted, suggested delay is one hour to give the administrator enough time
            to notice large number of pending deletion blocks and take corrective
            action.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.client.retry.policy.enabled</name>
        <value>false</value>
        <description>Enables HDFS client retry in the event of a NameNode failure.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.content-summary.limit</name>
        <value>5000</value>
        <description>Dfs content summary limit.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.encryption.key.provider.uri</name>
        <description>
            The KeyProvider to use when interacting with encryption keys used
            when reading and writing to an encryption zone.
        </description>
        <value/>
        <value-attributes>
            <empty-value-valid>true</empty-value-valid>
        </value-attributes>
        <depends-on>
            <property>
                <type>hadoop-env</type>
                <name>keyserver_host</name>
            </property>
            <property>
                <type>hadoop-env</type>
                <name>keyserver_port</name>
            </property>
            <property>
                <type>kms-env</type>
                <name>kms_port</name>
            </property>
            <property>
                <type>ranger-kms-site</type>
                <name>ranger.service.https.attrib.ssl.enabled</name>
            </property>
        </depends-on>
        <on-ambari-upgrade add="false"/>
    </property>

    <!-- These configs were inherited from HDP 2.3 -->
    <property>
        <name>nfs.file.dump.dir</name>
        <value>/tmp/.hdfs-nfs</value>
        <display-name>NFSGateway dump directory</display-name>
        <description>
            This directory is used to temporarily save out-of-order writes before
            writing to HDFS. For each file, the out-of-order writes are dumped after
            they are accumulated to exceed certain threshold (e.g., 1MB) in memory.
            One needs to make sure the directory has enough space.
        </description>
        <value-attributes>
            <type>directory</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>nfs.exports.allowed.hosts</name>
        <value>* rw</value>
        <description>
            By default, the export can be mounted by any client. To better control the access,
            users can update the following property. The value string contains machine name and access privilege,
            separated by whitespace characters. Machine name format can be single host, wildcards, and IPv4
            networks.The access privilege uses rw or ro to specify readwrite or readonly access of the machines
            to exports. If the access privilege is not provided, the default is read-only. Entries are separated
            by &quot;;&quot;. For example: &quot;192.168.0.0/22 rw ; host*.example.com ; host1.test.org ro;&quot;.
        </description>
        <display-name>Allowed hosts</display-name>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.encrypt.data.transfer.cipher.suites</name>
        <value>AES/CTR/NoPadding</value>
        <description>
            This value may be either undefined or AES/CTR/NoPadding. If defined, then
            dfs.encrypt.data.transfer uses the specified cipher suite for data encryption.
            If not defined, then only the algorithm specified in dfs.encrypt.data.transfer.algorithm
            is used. By default, the property is not defined.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>dfs.namenode.inode.attributes.provider.class</name>
        <description>Enable ranger hdfs plugin</description>
        <depends-on>
            <property>
                <type>ranger-hdfs-plugin-properties</type>
                <name>ranger-hdfs-plugin-enabled</name>
            </property>
        </depends-on>
        <value-attributes>
            <overridable>false</overridable>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.caller.context.enabled</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
        <supported-refresh-commands>
            <refresh-command componentName="NAMENODE" command="reload_configs"/>
        </supported-refresh-commands>
    </property>
    <property>
        <name>manage.include.files</name>
        <value>false</value>
        <description>If true Ambari will manage include file if dfs.hosts is configured.</description>
        <on-ambari-upgrade add="false"/>
    </property>

    <property>
        <name>dfs.balancer.dispatcherThreads</name>
        <value>5000</value>
    </property>
    <property>
        <name>dfs.balancer.moverThreads</name>
        <value>20000</value>
    </property>
    <property>
        <name>dfs.qjournal.start-segment.timeout.ms</name>
        <value>90000</value>
    </property>
    <property>
        <name>dfs.qjournal.select-input-streams.timeout.ms</name>
        <value>90000</value>
    </property>
    <property>
        <name>dfs.qjournal.write-txns.timeout.ms</name>
        <value>90000</value>
    </property>
    <property>
        <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
        <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
    </property>

    <property>
        <name>dfs.namenode.max.extra.edits.segments.retained</name>
        <value>10</value>
    </property>
    <property>
        <name>dfs.namenode.num.extra.edits.retained</name>
        <value>20</value>
    </property>

    <property>
        <name>dfs.datanode.fileio.profiling.sampling.fraction</name>
        <value>10</value>
    </property>

    <property>
        <name>dfs.datanode.peer.stats.enabled</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
    </property>

    <property>
        <name>dfs.datanode.max.locked.memory</name>
        <value>8589934592</value>
        <description>
            The amount of memory in bytes to use for caching of block replicas in memory on the datanode. This is 8G
        </description>
    </property>

    <property>
        <name>dfs.pipeline.ecn</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
    </property>

    <property>
        <name>dfs.datanode.du.reserved.ram_disk</name>
        <value>0</value>
        <description>datanode ram_disk reserved.</description>
    </property>

    <property>
        <name>dfs.datanode.du.reserved.pct.ram_disk</name>
        <value>0</value>
        <description>datanode ram_disk pct reserved.</description>
    </property>

    <property>
        <name>dfs.datanode.du.reserved.ssd</name>
        <value>0</value>
        <description>datanode ssd reserved.</description>
    </property>

    <property>
        <name>dfs.datanode.du.reserved.pct.ssd</name>
        <value>0</value>
        <description>datanode ssd pct reserved.</description>
    </property>


    <property>
        <name>dfs.permissions.ContentSummary.subAccess</name>
        <value>false</value>
        <depends-on>
            <property>
                <type>ranger-hdfs-plugin-properties</type>
                <name>ranger-hdfs-plugin-enabled</name>
            </property>
        </depends-on>
        <value-attributes>
            <overridable>false</overridable>
            <type>boolean</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>

    <property>
        <name>dfs.block.replicator.classname</name>
        <value>org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault</value>
        <description>
            Class representing block placement policy for non-striped files.
        </description>
    </property>

    <property>
        <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
        <value>200</value>
        <description>
            Advanced property. Change with caution. This determines the total amount of block transfers to begin in
            parallel at a DN, for replication, when such a command list is being sent over a DN heartbeat by the NN. The
            actual number is obtained by multiplying this multiplier with the total number of live nodes in the cluster.
            The result number is the number of blocks to begin transfers immediately for, per DN heartbeat. This number
            can be any positive, non-zero integer.
        </description>
    </property>

    <property>
        <name>dfs.namenode.replication.max-streams</name>
        <value>64</value>
        <description>
            Hard limit for the number of highest-priority replication streams.
        </description>
    </property>

    <property>
        <name>dfs.namenode.replication.max-streams-hard-limit</name>
        <value>128</value>
        <description>
            Hard limit for all replication streams.
        </description>
    </property>

    <property>
        <name>dfs.disk.balancer.max.disk.throughputInMBperSec</name>
        <value>600</value>
        <description>
            The maximum disk bandwidth that Disk Balancer,consumes while transferring data between disks. The default
            value is 10 MB/s
        </description>
    </property>

    <property>
        <name>dfs.namenode.ec.system.default.policy</name>
        <value>RS-10-4-1024k</value>
        <description>
            The default erasure coding policy name will be used on the path if no policy name is passed.
        </description>
    </property>

    <property>
        <name>dfs.blockreport.incremental.intervalMsec</name>
        <value>50</value>
        <description>
            the value in ms to wait between sending incremental block reports from the Datanode to the Namenode.
        </description>
    </property>

    <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
    </property>

    <property>
        <name>dfs.ha.tail-edits.in-progress</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
        <description>enable fast tailing on in-progress edit logs.</description>
    </property>
    <property>
          <name>dfs.namenode.state.context.enabled</name>
          <value>true</value>
          <description>
              Whether enable namenode sending back its current txnid back to client.
              Setting this to true is required by Consistent Read from Standby feature.But for regular cases,
              this should be set to false to avoid the overhead of updating and maintaining this state.
          </description>
    </property>
    <property>
        <name>dfs.ha.tail-edits.period</name>
        <value>1ms</value>
        <description>how often Standby/Observer NameNodes should fetch edits from JournalNodes.</description>
    </property>
    <property>
       <name>dfs.ha.tail-edits.period.backoff-max</name>
       <value>10s</value>
    </property>
    <property>
        <name>dfs.namenode.accesstime.precision</name>
        <value>0</value>
    </property>

    <property>
        <name>dfs.journalnode.edit-cache-size.bytes</name>
        <value>10485760</value>
        <description>the in-memory cache size, in bytes, on the JournalNodes.</description>
    </property>

    <property>
        <name>dfs.datanode.block-pinning.enabled</name>
        <value>true</value>
        <description>Whether pin blocks on favored DataNode.</description>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
        <description>Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration..</description>
    </property>
    <property>
        <name>dfs.datanode.directoryscan.threads</name>
        <value>6</value>
        <description>How many threads should the threadpool used to compile reports for volumes in parallel have.</description>
    </property>
    <property>
        <name>dfs.blockreport.incremental.intervalMsec</name>
        <value>30</value>
        <description>If set to a positive integer, the value in ms to wait between sending incremental block reports from the Datanode to the Namenode.</description>
    </property>
    <property>
        <name>dfs.datanode.cached-dfsused.check.interval.ms</name>
        <value>600000</value>
        <description>The interval check time of loading DU_CACHE_FILE in each volume. When the cluster doing the rolling upgrade operations, it will usually lead dfsUsed cache file of each volume expired and redo the du operations in datanode and that makes datanode start slowly. Adjust this property can make cache file be available for the time as you want.</description>
    </property>
   <property>
        <name>dfs.namenode.lock.detailed-metrics.enabled</name>
        <value>true</value>
        <description>If true, the namenode will keep track of how long various operations hold the Namesystem lock for and emit this as metrics.</description>
    </property>
   <property>
        <name>dfs.namenode.edits.asynclogging</name>
        <value>true</value>
        <description>If set to true, enables asynchronous edit logs in the Namenode. If set to false, the Namenode uses the traditional synchronous edit logs.</description>
    </property>

    <property>
        <name>dfs.namenode.quota.init-threads</name>
        <value>20</value>
        <description>
            The number of concurrent threads to be used in quota initialization. The
            speed of quota initialization also affects the namenode fail-over latency.
            If the size of name space is big, try increasing this.
        </description>
    </property>

    <property>
        <name>dfs.image.parallel.load</name>
        <value>true</value>
        <description>
            If true, write sub-section entries to the fsimage index so it can
            be loaded in parallel. Also controls whether parallel loading
            will be used for an image previously created with sub-sections.
            If the image contains sub-sections and this is set to false,
            parallel loading will not be used.
            Parallel loading is not compatible with image compression,
            so if dfs.image.compress is set to true this setting will be
            ignored and no parallel loading will occur.
        </description>
    </property>

    <property>
        <name>dfs.image.parallel.target.sections</name>
        <value>48</value>
        <description>
            Controls the number of sub-sections that will be written to
            fsimage for each section. This should be larger than
            dfs.image.parallel.threads, otherwise all threads will not be
            used when loading. Ideally, have at least twice the number
            of target sections as threads, so each thread must load more
            than one section to avoid one long running section affecting
            the load time.
        </description>
    </property>

    <property>
        <name>dfs.image.parallel.inode.threshold</name>
        <value>1000000</value>
        <description>
            If the image contains less inodes than this setting, then
            do not write sub-sections and hence disable parallel loading.
            This is because small images load very quickly in serial and
            parallel loading is not needed.
        </description>
    </property>

    <property>
        <name>dfs.image.parallel.threads</name>
        <value>16</value>
        <description>
            The number of threads to use when dfs.image.parallel.load is
            enabled. This setting should be less than
            dfs.image.parallel.target.sections. The optimal number of
            threads will depend on the hardware and environment.
        </description>
    </property>
</configuration>
