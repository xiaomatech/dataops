<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration xmlns:xi="http://www.w3.org/2001/XInclude" supports_final="true">
    <property>
        <name>ipc.server.listen.queue.size</name>
        <value>4096</value>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>ipc.server.log.slow.rpc</name>
        <value>true</value>
    </property>
    <property>
        <name>hadoop.http.cross-origin.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>hadoop.security.groups.cache.background.reload</name>
        <value>true</value>
    </property>
    <property>
        <name>hadoop.security.dns.log-slow-lookups.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>ipc.client.connect.timeout</name>
        <value>300000</value>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>ipc.client.connection.maxidletime</name>
        <value>3600000</value>
    </property>
    <property>
        <name>ipc.8020.backoff.enable</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
    </property>
    <property>
        <name>ipc.8020.callqueue.impl</name>
        <value>org.apache.hadoop.ipc.FairCallQueue</value>
    </property>
    <property>
        <name>ipc.8020.scheduler.impl</name>
        <value>org.apache.hadoop.ipc.DecayRpcScheduler</value>
    </property>
    <property>
        <name>ipc.8020.cost-provider.impl</name>
        <value>org.apache.hadoop.ipc.WeightedTimeCostProvider</value>
        <description>The cost provider mapping user requests to their cost</description>
    </property>
    <property>
      <name>ipc.8020.decay-scheduler.metrics.top.user.count</name>
      <value>10</value>
      <description>The number of top (i.e., heaviest) users to emit metric information about</description>
    </property>

    <property>
      <name>ipc.8020.weighted-cost.lockshared</name>
      <value>10</value>
      <description>The weight multiplier to apply to the time spent in the
        processing phase which holds a shared (read) lock.
        This property applies to WeightedTimeCostProvider.
      </description>
    </property>

    <property>
      <name>ipc.8020.weighted-cost.lockexclusive</name>
      <value>100</value>
      <description>The weight multiplier to apply to the time spent in the
        processing phase which holds an exclusive (write) lock.
        This property applies to WeightedTimeCostProvider.
      </description>
    </property>

    <property>
      <name>ipc.8020.weighted-cost.handler</name>
      <value>1</value>
      <description>The weight multiplier to apply to the time spent in the
        HANDLER phase which do not involve holding a lock.
        See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
        this phase. This property applies to WeightedTimeCostProvider.
      </description>
    </property>

    <property>
      <name>ipc.8020.weighted-cost.lockfree</name>
      <value>1</value>
      <description>The weight multiplier to apply to the time spent in the
        LOCKFREE phase which do not involve holding a lock.
        See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
        this phase
      </description>
    </property>

    <property>
      <name>ipc.8020.weighted-cost.response</name>
      <value>1</value>
      <description>The weight multiplier to apply to the time spent in the
        RESPONSE phase which do not involve holding a lock.
        See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on
        this phase
      </description>
    </property>
    <property>
        <name>ipc.8020.decay-scheduler.backoff.responsetime.enable</name>
        <value>true</value>
        <value-attributes>
            <type>boolean</type>
        </value-attributes>
    </property>
    <property>
        <name>ipc.8020.scheduler.priority.levels</name>
        <value>4</value>
    </property>

    <property>
        <name>hadoop.security.groups.cache.secs</name>
        <value>7200</value>
    </property>
    <property>
        <name>hadoop.security.groups.negative-cache.secs</name>
        <value>120</value>
    </property>

    <property>
        <name>ha.failover-controller.active-standby-elector.zk.op.retries</name>
        <value>120</value>
        <description>ZooKeeper Failover Controller retries setting for your environment</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <!-- i/o properties -->
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
        <description>The size of buffer for use in sequence files.
            The size of this buffer should probably be a multiple of hardware
            page size (4096 on Intel x86), and it determines how much data is
            buffered during read and write operations.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>io.serializations</name>
        <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
        <description>A list of comma-delimited serialization classes that can be used for obtaining serializers and
            deserializers.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec,org.apache.hadoop.io.compress.ZStandardCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
        <description>A list of the compression codec classes that can be used
            for compression/decompression.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <!-- file system properties -->
    <property>
        <name>fs.defaultFS</name>
        <!-- cluster variant -->
        <value>hdfs://localhost:8020</value>
        <description>The name of the default file system. Either the
            literal string "local" or a host:port for HDFS.
        </description>
        <final>true</final>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>360</value>
        <description>Number of minutes after which the checkpoint gets deleted.
            If zero, the trash feature is disabled.
            This option may be configured both on the server and the client.
            If trash is disabled server side then the client side configuration is checked.
            If trash is enabled on the server side then the value configured on the server is used and the client
            configuration value is ignored.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <!-- ipc properties: copied from kryptonite configuration -->
    <property>
        <name>ipc.client.idlethreshold</name>
        <value>8000</value>
        <description>Defines the threshold number of connections after which
            connections will be inspected for idleness.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>ipc.client.connection.maxidletime</name>
        <value>30000</value>
        <description>The maximum time after which a client will bring down the
            connection to the server.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>50</value>
        <description>Defines the maximum number of retries for IPC connections.</description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>ipc.server.tcpnodelay</name>
        <value>true</value>
        <description>Turn on/off Nagle's algorithm for the TCP socket
            connection on
            the server. Setting to true disables the algorithm and may
            decrease latency
            with a cost of more/smaller packets.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <!-- Web Interface Configuration -->
    <property>
        <name>mapreduce.jobtracker.webinterface.trusted</name>
        <value>false</value>
        <description>If set to true, the web interfaces of JT and NN may contain
            actions, such as kill job, delete file, etc., that should
            not be exposed to public. Enable this option if the interfaces
            are only reachable by those who have the right authorization.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.security.authentication</name>
        <value>simple</value>
        <description>
            Set the authentication for the cluster. Valid values are: simple or
            kerberos.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>false</value>
        <description>
            Enable authorization for different protocols.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>DEFAULT</value>
        <description>The mapping from kerberos principal names to local OS mapreduce.job.user.names.
So the default rule is just "DEFAULT" which takes all principals in your default domain to their first
component.
"omalley@APACHE.ORG" and "omalley/admin@APACHE.ORG" to "omalley", if your default domain is APACHE.ORG.
The translations rules have 3 sections:
base filter substitution
The base consists of a number that represents the number of components in the principal name excluding the
realm and the pattern for building the name from the sections of the principal name. The base uses $0 to
mean the realm, $1 to mean the first component and $2 to mean the second component.

[1:$1@$0] translates "omalley@APACHE.ORG" to "omalley@APACHE.ORG"
[2:$1] translates "omalley/admin@APACHE.ORG" to "omalley"
[2:$1%$2] translates "omalley/admin@APACHE.ORG" to "omalley%admin"

The filter is a regex in parens that must the generated string for the rule to apply.

"(.*%admin)" will take any string that ends in "%admin"
"(.*@ACME.COM)" will take any string that ends in "@ACME.COM"

Finally, the substitution is a sed rule to translate a regex into a fixed string.

"s/@ACME\.COM//" removes the first instance of "@ACME.COM".
"s/@[A-Z]*\.COM//" removes the first instance of "@" followed by a name followed by ".COM".
"s/X/Y/g" replaces all of the "X" in the name with "Y"

So, if your default realm was APACHE.ORG, but you also wanted to take all principals from ACME.COM that had
a single component "joe@ACME.COM", you'd do:

RULE:[1:$1@$0](.@ACME.ORG)s/@.//
DEFAULT

To also translate the names with a second component, you'd make the rules:

RULE:[1:$1@$0](.@ACME.ORG)s/@.//
RULE:[2:$1@$0](.@ACME.ORG)s/@.//
DEFAULT

If you want to treat all principals from APACHE.ORG with /admin as "admin", your rules would look like:

RULE[2:$1%$2@$0](.%admin@APACHE.ORG)s/./admin/
DEFAULT
        </description>
        <value-attributes>
            <type>multiLine</type>
        </value-attributes>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>net.topology.script.file.name</name>
        <value>/etc/hadoop/topology_script.py</value>
        <description>
            Location of topology script used by Hadoop to determine the rack location of nodes.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>

    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>true</value>
        <description>
            Indicates if anonymous requests are allowed when using &apos;simple&apos; authentication.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.security.key.provider.path</name>
        <value/>
        <value-attributes>
            <empty-value-valid>true</empty-value-valid>
        </value-attributes>
        <depends-on>
            <property>
                <type>hadoop-env</type>
                <name>keyserver_host</name>
            </property>
            <property>
                <type>hadoop-env</type>
                <name>keyserver_port</name>
            </property>
            <property>
                <type>kms-env</type>
                <name>kms_port</name>
            </property>
            <property>
                <type>ranger-kms-site</type>
                <name>ranger.service.https.attrib.ssl.enabled</name>
            </property>
        </depends-on>
        <on-ambari-upgrade add="false"/>
    </property>

    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer,org.apache.hadoop.security.HttpCrossOriginFilterInitializer</value>
        <description>A comma separated list of class names. Each class in the list must extend
            org.apache.hadoop.http.FilterInitializer.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>

    <!-- CORS properties-->
    <property>
        <name>hadoop.http.cross-origin.allowed-origins</name>
        <value>*</value>
        <description>Comma separated list of origins that are allowed for web services
            needing cross-origin (CORS) support.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.http.cross-origin.allowed-methods</name>
        <value>GET,PUT,POST,OPTIONS,HEAD,DELETE</value>
        <description>Comma separated list of methods that are allowed for web
            services needing cross-origin (CORS) support.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.http.cross-origin.allowed-headers</name>
        <value>X-Requested-With,Content-Type,Accept,Origin,WWW-Authenticate,Accept-Encoding,Transfer-Encoding</value>
        <description>Comma separated list of headers that are allowed for web
            services needing cross-origin (CORS) support.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.http.cross-origin.max-age</name>
        <value>1800</value>
        <description>The number of seconds a pre-flighted request can be cached
            for web services needing cross-origin (CORS) support.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>

    <property>
        <name>fs.s3a.fast.upload</name>
        <value>true</value>
        <description>
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>fs.s3a.fast.upload.buffer</name>
        <value>disk</value>
        <description>
The buffering mechanism to use.
Values: disk, array, bytebuffer.

"disk" will use the directories listed in fs.s3a.buffer.dir as
the location(s) to save data prior to being uploaded.

"array" uses arrays in the JVM heap

"bytebuffer" uses off-heap memory within the JVM.

Both "array" and "bytebuffer" will consume memory in a single stream up to the number
of blocks set by:

fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks.

If using either of these mechanisms, keep this value low

The total number of threads performing work across all threads is set by
fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued
work items.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>fs.s3a.multipart.size</name>
        <value>67108864</value>
        <description>
            How big (in bytes) to split upload or copy operations up into.
            A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.
        </description>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>hadoop.security.instrumentation.requires.admin</name>
        <value>false</value>
        <description>
            Indicates if administrator ACLs are required to access
            instrumentation servlets (JMX, METRICS, CONF, STACKS).
        </description>
        <on-ambari-upgrade add="false"/>
    </property>


    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
    <property>
        <name>ipc.server.listen.queue.size</name>
        <value>256</value>
    </property>
    <property>
        <name>ipc.client.connect.timeout</name>
        <value>90000</value>
    </property>

    <property>
        <name>dfs.client.failover.random.order</name>
        <value>true</value>
    </property>

    <property>
        <name>hadoop.shell.safely.delete.limit.num.files</name>
        <value>100</value>
        <description>Used by -safely option of hadoop fs shell -rm command to avoid
            accidental deletion of large directories.
        </description>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>/tmp/hadoop-${user.name}</value>
        <description>A base for other temporary directories.</description>
    </property>

    <property>
        <name>hadoop.security.group.mapping.ldap.search.filter.user</name>
        <value>(&amp;(objectClass=posixAccount)(uid={0}))</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.filter.group</name>
        <value>(|(objectClass=posixGroup)(objectClass=groupOfNames)(objectClass=group))</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.attr.member</name>
        <value>memberuid</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.attr.group.name</name>
        <value>cn</value>
    </property>

</configuration>
